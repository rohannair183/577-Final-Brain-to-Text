# CNN + Transformer encoder configuration
experiment:
  name: "cnn_transformer_encoder"
  description: "Convolutional front-end with Transformer encoder + CTC loss"
  seed: 42

data:
  data_root: "data/hdf5_data_final"
  train_sessions:
    - "t15.2023.08.11"
    - "t15.2023.08.13"
    - "t15.2023.08.18"
    - "t15.2023.08.20"
  val_sessions:
    - "t15.2023.09.24"
  test_sessions:
    - "t15.2023.10.01"
  batch_size: 12
  num_workers: 0
  pin_memory: false
  shuffle_train: true

model:
  type: "cnn_transformer"
  input_dim: 512
  num_phonemes: 41
  cnn_channels: [256, 256, 256]
  cnn_kernel_sizes: [5, 3, 3]
  cnn_strides: [2, 1, 1]   # first layer subsamples time for cheaper Transformer
  d_model: 320
  num_layers: 6
  num_heads: 4
  dim_feedforward: 1280
  dropout: 0.10
  max_len: 6000
  blank_penalty: 2.5
  blank_id: 0

training:
  num_epochs: 5
  loss:
    type: "ctc"
    blank_id: 0
  optimizer:
    type: "adamw"
    learning_rate: 0.0006
    weight_decay: 0.003
  scheduler:
    type: "cosine"
    t_max: 50
    eta_min: 1.0e-6
  gradient_clip: 0.5
  checkpoint_dir: "experiments/cnn_transformer_encoder"
  save_every_n_epochs: 5
  keep_last_n_checkpoints: 3

evaluation:
  validate_every_n_epochs: 1
  compute_per: true
  compute_wer: false
  compute_cer: false

logging:
  log_dir: "experiments/cnn_transformer_encoder/logs"
  log_every_n_steps: 50
  use_tensorboard: true
  use_wandb: false
  log_n_predictions: 5

compute:
  device: "cuda"
  mixed_precision: false
  cudnn_benchmark: true
  deterministic: false

debug:
  fast_dev_run: false
  overfit_batches: 0
  profile: false
  detect_anomaly: false
  check_data_once: true
